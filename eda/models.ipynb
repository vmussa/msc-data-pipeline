{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de modelos LDA para análise de tópicos em comentários do subreddit r/mturk\n",
    "Este notebook treina uma série de modelos de tópicos a serem usados numa análise exploratória corpus de comentários do subreddit /r/mturk coletado para a dissertação através do `/tracer`.\n",
    "Faremos isso com a ajuda do LDA (Latent Dirichlet Allocation, treinando diferentes versões com uma variede de parâmetros.\n",
    "Os modelos treinados serão usados como ferramenta de análise exploratória. No notebook `vis.ipynb`, faremos a visualização dos tópicos encontrados, e análise dos tópicos será feita na dissertação."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2dgn6km56qb307n7j1k2_jyr0000gn/T/ipykernel_98971/815672776.py:3: DtypeWarning: Columns (7,11,23,25,26,29,32,34,37,38,40,46,49,50,54,56,57,59,60,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/corpus.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1470773823</td>\n",
       "      <td>[This](https://greasyfork.org/en/scripts/10615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1470773732</td>\n",
       "      <td>Can I ask where are you from? I'm a worker fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1470773686</td>\n",
       "      <td>2.4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1470773635</td>\n",
       "      <td>Which version are you using?  I have settings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1470773208</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc                                               body\n",
       "0   1470773823  [This](https://greasyfork.org/en/scripts/10615...\n",
       "1   1470773732  Can I ask where are you from? I'm a worker fro...\n",
       "2   1470773686                                              2.4.9\n",
       "3   1470773635  Which version are you using?  I have settings ...\n",
       "4   1470773208                                          [deleted]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/corpus.csv')\n",
    "df = df[['created_utc', 'body']]\n",
    "df = df.dropna(subset=['body'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento com LDA\n",
    "Usamos as bibliotecas `gensim` e `nltk` para fazer o treinamento dos modelos LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vitorgomes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vitorgomes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from gensim import models\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento de texto: remoção de stopwords e pontuação, lematização)\n",
    "\n",
    "Neste passo, pré-processamos os dados de texto removendo stopwords e pontuação, e lematizando as palavras.\n",
    "Também removemos informações sensíveis como nomes de usuários e URLs, minimizando o risco de re-identificação e violações de privacidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "\n",
    "    # Passo 1: Remove URLs due to privacy concerns\n",
    "    doc = re.sub(r\"http[s]?://\\S+|www\\.\\S+\", \"\", doc)\n",
    "    # Passo 2: Remove Reddit-style usernames (e.g., u/username) (privacy concerns)\n",
    "    doc = re.sub(r\"\\bu/\\w+\\b\", \"\", doc)\n",
    "    # Passo 3: Remove email addresses (privacy concerns)\n",
    "\n",
    "    # Limpeza final: Remove stopwords/pontuação e lematiza palavras\n",
    "    doc = re.sub(r\"\\b\\w+@\\w+\\.\\w+\\b\", \"\", doc)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in df['body']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando o dicionário e o corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Cria uma representação de dicionário dos documentos\n",
    "dictionary = Dictionary(doc_clean)\n",
    "\n",
    "# Filtra o dicionário para remover termos que aparecem em menos de 2 documentos e em mais de 95% dos documentos\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.95)\n",
    "\n",
    "# Representa os documentos na forma bag-of-words\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo LDA com 10 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_10t = models.LdaModel(corpus=corpus, num_topics=10, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo LDA com 100 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_100t = models.LdaModel(corpus=corpus, num_topics=100, id2word=dictionary, passes=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os modelos para o disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import MmCorpus\n",
    "\n",
    "os.chdir(\"../eda\")\n",
    "\n",
    "dictionary.save('./models/dictionary.gensim')\n",
    "MmCorpus.serialize('./models/corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_10t.save('models/lda_10t/lda_10t.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_100t.save('models/lda_100t/lda_100t.gensim')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
